{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modular-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prospective-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Parallelize_DataFrame import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "patent-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-whale",
   "metadata": {},
   "source": [
    "### 마이비 카드 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respective-blame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 7.55 s, total: 32.6 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mybicard = pd.read_parquet('/home/seho/Passenger_Demand/data/mybicard.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "provincial-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# mybicard = mybicard.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "musical-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수집일자 데이트 포맷으로 변환\n",
    "mybicard[\"collectdate\"] = pd.to_datetime(mybicard[\"collectdate\"], format = \"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "general-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전송일자 데이트 포맷으로 변환\n",
    "mybicard[\"transdate\"] = pd.to_datetime(mybicard[\"transdate\"], format = \"%Y%m%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "olive-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 승객 수 변수 생성(일반 + 학생 + 아동)\n",
    "mybicard[\"totalcnt\"] = mybicard[[\"normalcnt\", \"studentcnt\", \"childcnt\"]].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "separated-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_nm에 공백이 포함되어 있어 공백 제거\n",
    "mybicard[\"route_nm\"] = mybicard[\"route_nm\"].replace(\"\\s\", \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "welsh-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ;mybicard = mybicard.sort_values([\"transdate\", \"seq\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "middle-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard = mybicard.rename(columns = {\"stop_id\" : \"mybi_stop_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-framework",
   "metadata": {},
   "source": [
    "### 401번 버스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "academic-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401 = mybicard.loc[(mybicard[\"route_nm\"] == \"401\")].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "better-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401_agg = (mybicard_401.loc[mybicard_401[\"transflag\"] != \"하차\"]\n",
    "                                   .groupby([\"mybi_stop_id\", pd.Grouper(key=\"transdate\", freq='60Min')])\n",
    "                                   .agg(normalcnt = (\"normalcnt\",sum), \n",
    "                                        studentcnt = (\"studentcnt\", sum), \n",
    "                                        childcnt = (\"childcnt\", sum),\n",
    "                                        totalcnt = (\"totalcnt\", sum))\n",
    "                                   .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "handy-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401_agg.to_parquet(\"/home/seho/Passenger_Demand/data/mybicard_401_agg.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-conspiracy",
   "metadata": {},
   "source": [
    "### 결측치 \n",
    "하루의 수집 데이터의 수가 0인 날짜의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "complete-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_date = mybicard_401_agg.groupby([pd.Grouper(key=\"transdate\", freq=\"1D\")]).size().reset_index(name = \"cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "prepared-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_date = count_by_date.loc[count_by_date[\"cnt\"] == 0, \"transdate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-railway",
   "metadata": {},
   "source": [
    "#### n주 전 같은 요일 같은 시간대의 인원 수로 Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "activated-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_recent_data(data, missing_date, date_col = \"transdate\"):\n",
    "    data = data.copy()\n",
    "    for x in missing_date:\n",
    "        temp = []\n",
    "        w = 0\n",
    "        while len(temp) == 0:\n",
    "            w +=1\n",
    "            temp = data.loc[data[date_col].dt.date == (x - timedelta(weeks = w)).date()].copy()\n",
    "\n",
    "        temp[date_col] = temp[date_col] + timedelta(weeks = w)\n",
    "        data = pd.concat([data, temp], 0)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-praise",
   "metadata": {},
   "source": [
    "#### 최근 n주의 같은 요일 같은 시간대의 평균값으로 Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brown-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_recent_mean_data(data, missing_date, date_col):\n",
    "\n",
    "    data = data.copy()\n",
    "\n",
    "    # 요일, 시간 추가\n",
    "    data[\"dayofweek\"] = data[\"transdate\"].dt.dayofweek\n",
    "    dow_dict = {0:\"월\", 1:\"화\", 2:\"수\", 3:\"목\", 4:\"금\", 5:\"토\", 6:\"일\"}\n",
    "    data[\"dayofweek\"] = data[\"dayofweek\"].replace(dow_dict)\n",
    "    data[\"hour\"] = data[\"transdate\"].dt.hour\n",
    "\n",
    "    for x in missing_date:\n",
    "        base_date = x\n",
    "        w = 0\n",
    "        # 결측일의 이전 4주를 기본으로 검색하며, 데이터가 없는 경우 범위를 1주씩 늘려가며 데이터 조회\n",
    "        temp = []\n",
    "        while len(temp) == 0:\n",
    "            temp = data.loc[(data[\"transdate\"].dt.date.between((x - timedelta(weeks = 4+w)).date(), x.date())) & (data[\"transdate\"].dt.dayofweek == x.day_of_week)].copy()\n",
    "            w += 1\n",
    "\n",
    "        # 4+w 전까지의 데이터를 찾아서 정류장별, 요일별, 시간별 평균값 산출\n",
    "        temp2 = temp.groupby([\"mybi_stop_id\", \"dayofweek\", \"hour\"]).agg({\"totalcnt\" : np.mean,\n",
    "                                                                         \"normalcnt\" : np.mean,\n",
    "                                                                         \"studentcnt\" : np.mean,\n",
    "                                                                         \"childcnt\" : np.mean}).reset_index()\n",
    "        # 평균값 변환 (Float -> Int : 반올림 효과)\n",
    "        temp2[\"totalcnt\"] = temp2[\"totalcnt\"].astype(int)\n",
    "        temp2[\"normalcnt\"] = temp2[\"normalcnt\"].astype(int)\n",
    "        temp2[\"studentcnt\"] = temp2[\"studentcnt\"].astype(int)\n",
    "        temp2[\"childcnt\"] = temp2[\"childcnt\"].astype(int)\n",
    "\n",
    "        # 기준 일자, 시간으로 부터 transdate을 재생성\n",
    "        temp2[\"transdate\"] = temp2.apply(lambda x: base_date + timedelta(hours = x[\"hour\"]), 1)\n",
    "\n",
    "        data = pd.concat([data, temp2], 0)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "drawn-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401_agg_impute = impute_recent_mean_data(data = mybicard_401_agg, missing_date = missing_date, date_col = \"transdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "macro-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mybicard_401_agg = impute_recent_data(data = mybicard_401_agg, missing_date = missing_date, date_col = \"transdate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-ownership",
   "metadata": {},
   "source": [
    "#### 결측일을 제외한 결측치(특정 시간에 데이터가 없는 경우)는 승객이 0이므로 0으로 대체한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "attractive-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 시작과 끝 사이를 1시간 간격으로 구분하여 list 생성\n",
    "dt_list = pd.date_range(start = mybicard_401_agg_impute[\"transdate\"].min(), end = mybicard_401_agg_impute[\"transdate\"].max(), freq = \"1h\")\n",
    "\n",
    "transdate_df = pd.DataFrame({\"transdate\" : dt_list}).reset_index(drop = True)\n",
    "mybi_stop_id_df = pd.DataFrame({\"mybi_stop_id\" : mybicard_401_agg_impute[\"mybi_stop_id\"].drop_duplicates()}).reset_index(drop = True)\n",
    "\n",
    "# 전체 일정(시간 단위)과 정류소 별 조합 DF 생성\n",
    "all_date = pd.merge(transdate_df, mybi_stop_id_df, how = \"cross\")\n",
    "\n",
    "# 결측일의 데이터를 채워넣은 전체 데이터를 left join\n",
    "ml_data = pd.merge(all_date, mybicard_401_agg_impute, on = [\"mybi_stop_id\", \"transdate\"], how = \"left\")\n",
    "\n",
    "ml_data[\"dayofweek\"] = ml_data[\"transdate\"].dt.dayofweek\n",
    "dow_dict = {0:\"월\", 1:\"화\", 2:\"수\", 3:\"목\", 4:\"금\", 5:\"토\", 6:\"일\"}\n",
    "ml_data[\"dayofweek\"] = ml_data[\"dayofweek\"].replace(dow_dict)\n",
    "ml_data[\"hour\"] = ml_data[\"transdate\"].dt.hour\n",
    "# 결측일이 아닌 결측값은 승객수가 없다고 판단하여 0으로 대체\n",
    "ml_data = ml_data.fillna(0)\n",
    "ml_data = ml_data.loc[ml_data[\"hour\"].isin([1,2,3,4]) == False ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cutting-relationship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 8)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-allen",
   "metadata": {},
   "source": [
    "### 시계열 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "domestic-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_feature(data, target_cols, date_cols, lags, groupby_cols = None):\n",
    "    data = data.copy()\n",
    "    if isinstance(lags, list) == False:\n",
    "        lags = [lags]\n",
    "    if isinstance(date_cols, list) == False:\n",
    "        date_cols = [date_cols]\n",
    "    if isinstance(target_cols, list) == False:\n",
    "        target_cols = [target_cols]\n",
    "    if isinstance(groupby_cols, list) == False:\n",
    "        groupby_cols = [groupby_cols]\n",
    "    \n",
    "    for lg in lags:\n",
    "        if groupby_cols is None:\n",
    "            cnt_bf = data.set_index(date_cols)[target_cols].shift(freq = lg).reset_index()\n",
    "        else:\n",
    "            cnt_bf = data.set_index(date_cols).groupby(groupby_cols)[target_cols].shift(freq = lg).reset_index()\n",
    "        \n",
    "        rename_dict = {col: f\"{col}_bf{lg}\" for col in target_cols}\n",
    "        cnt_bf = cnt_bf.rename(columns = rename_dict)\n",
    "        \n",
    "        data = pd.merge(data, cnt_bf, on = date_cols + groupby_cols, how = \"left\")\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "daily-terror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 s, sys: 9.14 ms, total: 1.27 s\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ml_data = create_lag_feature(data = ml_data, target_cols = \"totalcnt\", date_cols = \"transdate\", lags = [\"1d\", \"2d\", \"3d\", \"4d\", \"5d\", \"6d\", \"7d\"], groupby_cols = \"mybi_stop_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "featured-valuation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 15)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-witch",
   "metadata": {},
   "source": [
    "### 날짜별 평균 Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "independent-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401_agg_daily = (ml_data.groupby([\"mybi_stop_id\", pd.Grouper(key=\"transdate\", freq='1d')])\n",
    "                                   .agg(normalcnt = (\"normalcnt\", np.mean), \n",
    "                                        studentcnt = (\"studentcnt\", np.mean), \n",
    "                                        childcnt = (\"childcnt\", np.mean),\n",
    "                                        totalcnt = (\"totalcnt\", np.mean))\n",
    "                                   .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "regulated-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_lag = create_lag_feature(data = mybicard_401_agg_daily, target_cols = \"totalcnt\", date_cols = \"transdate\", lags = [\"1d\", \"2d\", \"3d\", \"4d\", \"5d\", \"6d\", \"7d\"], groupby_cols = \"mybi_stop_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "forbidden-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [\"1d\", \"2d\", \"3d\", \"4d\", \"5d\", \"6d\", \"7d\"]\n",
    "rename_dict = {f\"{col}_bf{lg}\": f\"{col}_bf{lg}_total\" for col in [\"totalcnt\"] for lg in lags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "descending-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_lag = daily_lag.rename(columns = rename_dict)\n",
    "daily_lag[\"date\"] = daily_lag[\"transdate\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "prerequisite-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data[\"date\"] = ml_data[\"transdate\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "athletic-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(ml_data, daily_lag[[\"date\", \"mybi_stop_id\"] + list(rename_dict.values())], on = [\"date\", \"mybi_stop_id\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "invisible-arlington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 23)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-feature",
   "metadata": {},
   "source": [
    "### Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-pathology",
   "metadata": {},
   "source": [
    "#### 1) 이전 n개일자들의 동일 시간대 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "recreational-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_agg(data, target_cols, date_col, groupby_cols, col_nm = \"\", rollings = [\"2d\"], agg_func = [np.mean, np.std]):\n",
    "    if isinstance(target_cols, list) == False:\n",
    "        target_cols = [target_cols]\n",
    "        \n",
    "    if isinstance(groupby_cols, list) == False:\n",
    "        groupby_cols = [groupby_cols]\n",
    "        \n",
    "    if col_nm != \"\":\n",
    "        col_nm = f\"{col_nm}_\"\n",
    "    \n",
    "    \n",
    "    for rl in rollings:\n",
    "        for tg in target_cols:\n",
    "            data = data.set_index(date_col).sort_index(ascending=True).copy()\n",
    "            rolling_data = data.groupby(groupby_cols)[tg].rolling(rl).agg(agg_func)\n",
    "            rolling_data = rolling_data.rename(columns = {\"mean\" : f\"{tg}_ma_{col_nm}mean_{rl}\", \n",
    "                                                          \"std\" : f\"{tg}_ma_{col_nm}std_{rl}\"})\n",
    "            rolling_data = rolling_data.groupby(groupby_cols).shift(1).reset_index()    \n",
    "            \n",
    "            data = pd.merge(data.reset_index(), rolling_data, on = [date_col] + groupby_cols, how = \"left\")\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "protecting-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 s, sys: 46.7 ms, total: 2.09 s\n",
      "Wall time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ml_data = calculate_moving_agg(data = ml_data, target_cols = [\"totalcnt\"], date_col = \"transdate\", groupby_cols = [\"mybi_stop_id\", \"hour\"], col_nm = \"hour\", rollings = [\"2d\", \"3d\", \"4d\", \"5d\", \"6d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "starting-mathematics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 33)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-watts",
   "metadata": {},
   "source": [
    "#### 2) n주전까지의 동일 요일의 동일 시간대 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "unique-texture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.12 s, sys: 28.4 ms, total: 3.15 s\n",
      "Wall time: 3.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ml_data = calculate_moving_agg(data = ml_data, target_cols = [\"totalcnt\"], date_col = \"transdate\", groupby_cols = [\"mybi_stop_id\", \"hour\", \"dayofweek\"], col_nm = \"hour_week\", rollings = [\"14d\", \"21d\", \"28d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "recognized-peripheral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 39)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-pavilion",
   "metadata": {},
   "source": [
    "#### 3) 이전 n개일자들의 전체 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "generic-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401_agg_1d = (ml_data.groupby([\"mybi_stop_id\", pd.Grouper(key=\"transdate\", freq='1d')])\n",
    "                                   .agg(normalcnt = (\"normalcnt\",sum), \n",
    "                                        studentcnt = (\"studentcnt\", sum), \n",
    "                                        childcnt = (\"childcnt\", sum),\n",
    "                                        totalcnt = (\"totalcnt\", sum))\n",
    "                                   .reset_index())\n",
    "\n",
    "mybicard_401_agg_1d[\"dayofweek\"] = mybicard_401_agg_1d[\"transdate\"].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fundamental-sharing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 110 ms, sys: 0 ns, total: 110 ms\n",
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daily_agg = calculate_moving_agg(data = mybicard_401_agg_1d, target_cols = [\"totalcnt\"], date_col = \"transdate\", groupby_cols = [\"mybi_stop_id\"], col_nm = \"daily\", rollings = [\"2d\", \"3d\", \"4d\", \"5d\", \"6d\"])\n",
    "daily_agg[\"date\"] = daily_agg[\"transdate\"].dt.date\n",
    "daily_agg = daily_agg.drop([\"transdate\", \"normalcnt\", \"studentcnt\", \"childcnt\", \"totalcnt\", \"dayofweek\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "invalid-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(ml_data, daily_agg, on = [\"mybi_stop_id\", \"date\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "dangerous-processing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 49)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-receptor",
   "metadata": {},
   "source": [
    "#### 4) n주전까지의 동일 요일의 전체 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "automatic-arrival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 814 µs, total: 137 ms\n",
      "Wall time: 137 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daily_week_agg = calculate_moving_agg(data = mybicard_401_agg_1d, target_cols = [\"totalcnt\"], date_col = \"transdate\", groupby_cols = [\"mybi_stop_id\", \"dayofweek\"], col_nm = \"daily_week\", rollings = [\"14d\", \"21d\", \"28d\"])\n",
    "daily_week_agg[\"date\"] = daily_week_agg[\"transdate\"].dt.date\n",
    "daily_week_agg = daily_week_agg.drop([\"transdate\", \"normalcnt\", \"studentcnt\", \"childcnt\", \"totalcnt\", \"dayofweek\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "pediatric-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(ml_data, daily_week_agg, on = [\"mybi_stop_id\", \"date\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "existing-montreal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 55)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-roads",
   "metadata": {},
   "source": [
    "#### 5) n주전까지의 주 평균의 이동평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "weekly-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401_agg_1d = (ml_data.groupby([\"mybi_stop_id\", pd.Grouper(key=\"transdate\", freq='1d')])\n",
    "                                   .agg(normalcnt = (\"normalcnt\",sum), \n",
    "                                        studentcnt = (\"studentcnt\", sum), \n",
    "                                        childcnt = (\"childcnt\", sum),\n",
    "                                        totalcnt = (\"totalcnt\", sum))\n",
    "                                   .reset_index())\n",
    "mybicard_401_agg_1d[\"hour\"] = mybicard_401_agg_1d[\"transdate\"].dt.hour\n",
    "mybicard_401_agg_1d[\"weekofyear\"] = mybicard_401_agg_1d[\"transdate\"].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bright-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401_agg_1w = mybicard_401_agg_1d.groupby([\"mybi_stop_id\", \"weekofyear\"])[\"totalcnt\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "southwest-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_agg = calculate_moving_agg(data = mybicard_401_agg_1w, target_cols = [\"totalcnt\"], date_col = \"weekofyear\", groupby_cols = [\"mybi_stop_id\"], col_nm = \"weekly\", rollings = [2,3,4])\n",
    "weekly_agg = weekly_agg.drop(\"totalcnt\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "golden-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data[\"weekofyear\"] = ml_data[\"transdate\"].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "union-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(ml_data, weekly_agg, on = [\"mybi_stop_id\", \"weekofyear\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "commercial-auckland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 62)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-leisure",
   "metadata": {},
   "source": [
    "### 특일 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "planned-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_data = pd.read_parquet(\"/home/seho/Passenger_Demand/data/holiday_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "radical-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_data[\"date\"] = pd.to_datetime(holiday_data[\"locdate\"], format = \"%Y%m%d\").dt.date\n",
    "holiday_data = holiday_data.drop([\"locdate\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "sorted-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = pd.DataFrame({\"date\" : pd.date_range(\"2020-01-01\", \"2020-12-31\", freq = \"1D\")})\n",
    "date_df[\"weekend\"] = np.where(date_df[\"date\"].dt.dayofweek.isin([5,6]), \"Y\", \"N\")\n",
    "date_df[\"date\"] = date_df[\"date\"].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-wheat",
   "metadata": {},
   "source": [
    "#### 공휴일 / 명절 여부 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "cooperative-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공휴일(holiday), 명절(ntl_holiday) 구분\n",
    "ntl_holiday = holiday_data.loc[holiday_data[\"dateName\"].isin([\"설날\", \"추석\"])]\n",
    "ntl_holiday = ntl_holiday.rename(columns = {\"dateName\" : \"ntl_holi\"})\n",
    "\n",
    "holiday = holiday_data.loc[holiday_data[\"dateName\"].isin([\"설날\", \"추석\"]) == False]\n",
    "holiday = holiday.rename(columns = {\"dateName\" : \"holi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "traditional-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = pd.merge(date_df, ntl_holiday, on = \"date\", how = \"left\")\n",
    "date_df = pd.merge(date_df, holiday, on = \"date\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "speaking-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = date_df.assign(ntl_holi = np.where(date_df[\"ntl_holi\"].isna(), \"N\", \"Y\"),\n",
    "                         holi = np.where(date_df[\"holi\"].isna(), \"N\", \"Y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-simpson",
   "metadata": {},
   "source": [
    "#### 3일 이상 연휴 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "applicable-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df[\"rest_yn\"] = date_df[[\"weekend\", \"ntl_holi\", \"holi\"]].apply(lambda x: any(x == \"Y\"), 1)\n",
    "date_df[\"rest_yn\"] = np.where(date_df[\"rest_yn\"],\"Y\", \"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "intermediate-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seq_y(data, criterion = 2):\n",
    "    seq_list = []\n",
    "    Y_cnt = 0\n",
    "    for i, x in enumerate(data):\n",
    "        if x == \"Y\":\n",
    "            Y_cnt += 1\n",
    "\n",
    "        if (x == \"N\") | (i == len(data)):\n",
    "            if Y_cnt > criterion:\n",
    "                temp_list = [\"Y\"] * Y_cnt\n",
    "                seq_list += temp_list\n",
    "            elif (Y_cnt > 0) & (Y_cnt <= criterion):\n",
    "                temp_list = [\"N\"] * Y_cnt\n",
    "                seq_list += temp_list\n",
    "            seq_list.append(\"N\")\n",
    "            Y_cnt = 0\n",
    "            \n",
    "    return seq_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "reflected-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df[\"seq_holi\"] = find_seq_y(data = date_df[\"rest_yn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "adolescent-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = date_df.drop([\"weekend\", \"rest_yn\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "afraid-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(ml_data, date_df, on = \"date\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "indirect-consequence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 65)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-bahamas",
   "metadata": {},
   "source": [
    "### 날씨 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "precise-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_parquet(\"/home/seho/Passenger_Demand/data/weather_2018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "automotive-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data.loc[:, [\"tm\", \"ta\", \"hm\", \"rn\", \"dc10Tca\",  \"dsnw\", \"wd\", \"ws\"]]\n",
    "weather_data = weather_data.rename(columns = {\"tm\" : \"time\",\n",
    "                                              \"ta\" : \"temperature\",\n",
    "                                              \"hm\" : \"humidity\",\n",
    "                                              \"rn\" : \"precipitation\",\n",
    "                                              \"dc10Tca\" : \"전운량\",\n",
    "                                              \"dsnw\" : \"snowfall\",\n",
    "                                              \"wd\" : \"풍향\",\n",
    "                                              \"ws\" : \"풍속\"})\n",
    "weather_data[\"time\"] = pd.to_datetime(weather_data[\"time\"], format = \"%Y-%m-%d %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "silent-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in weather_data.columns:\n",
    "    if col == \"time\":\n",
    "        continue\n",
    "    weather_data[col] = weather_data[col].replace(\"\", \"0.0\").astype(float)\n",
    "    weather_data[col] = weather_data[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "private-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data[\"time_hours\"] = weather_data[\"time\"].dt.strftime(\"%Y-%m-%d %H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "governing-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data[\"transdate_hours\"] = ml_data[\"transdate\"].dt.strftime(\"%Y-%m-%d %H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "scenic-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(ml_data, weather_data[[\"time_hours\", \"temperature\", \"humidity\", \"precipitation\", \"snowfall\"]], left_on = \"transdate_hours\", right_on = \"time_hours\")\n",
    "ml_data = ml_data.drop([\"transdate_hours\", \"time_hours\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "developed-entertainment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 69)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-webster",
   "metadata": {},
   "source": [
    "### 미세먼지 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "prescribed-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data = pd.read_csv(\"/home/seho/Passenger_Demand/data/pm_data.csv\")\n",
    "pm_data[\"issueDate\"] = pd.to_datetime(pm_data[\"issueDate\"], format = \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "treated-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_agg = pm_data.loc[pm_data[\"districtName\"] == \"울산\"].groupby(pd.Grouper(key=\"issueDate\", freq=\"1D\")).size().reset_index(name = \"pm_alert_cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "executive-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data[\"date\"] = ml_data[\"transdate\"].dt.strftime(\"%Y-%m-%d\")\n",
    "pm_data_agg[\"issueDate\"] = pm_data_agg[\"issueDate\"].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "delayed-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(ml_data, pm_data_agg, how = \"left\", left_on = \"date\", right_on = \"issueDate\")\n",
    "ml_data = ml_data.drop([\"issueDate\"], 1)\n",
    "ml_data[\"pm_alert_cnt\"] = ml_data[\"pm_alert_cnt\"].fillna(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "another-future",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 70)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-andorra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-breakfast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sharp-optics",
   "metadata": {},
   "source": [
    "### 정류장 X,Y 좌표 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "dutch-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경주시, 양산시, 울산광역시, 부산광역시\n",
    "bus_stop_info = pd.read_csv(\"/home/seho/Passenger_Demand/data/울산광역시_버스 정류소 위치 정보_20200531.csv\", encoding = \"euc-kr\")\n",
    "bus_stop_info = bus_stop_info.loc[bus_stop_info[\"권역\"] == \"울산광역시\"]\n",
    "bus_stop_info.columns = [\"stop_nm\", \"stop_id\", \"longitude\", \"latitude\", \"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "opening-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_401_1 = pd.read_csv(\"/home/seho/Passenger_Demand/data/401_율리_꽃바위.csv\", encoding = \"euc_kr\")\n",
    "bus_stop_401_2 = pd.read_csv(\"/home/seho/Passenger_Demand/data/401_꽃바위_율리.csv\", encoding = \"euc_kr\")\n",
    "bus_stop_401 = pd.concat([bus_stop_401_1, bus_stop_401_2])\n",
    "bus_stop_401.columns = [\"mybi_stop_id\", \"stop_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "imperial-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_401_info = pd.merge(bus_stop_401, bus_stop_info, on = \"stop_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-valuable",
   "metadata": {},
   "source": [
    "### 상권정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "russian-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_area = pd.read_csv(\"/home/seho/Passenger_Demand/data/울산광역시_상권정보_201231.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "promotional-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = trading_area[\"상권업종중분류명\"].drop_duplicates().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "nonprofit-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(latlon1, latlon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lat1, lon1 = map(radians, latlon1)\n",
    "    lat2, lon2 = map(radians, latlon2)\n",
    "#     lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def count_store_nearby(data, trading_area = trading_area, dist = 0.1, category_list = None):\n",
    "    data_copy = data.copy()\n",
    "    if category_list == None:\n",
    "        category_list = trading_area[\"상권업종중분류명\"].drop_duplicates().to_list()\n",
    "    \n",
    "    dist_list = trading_area[[\"위도\", \"경도\"]].apply(lambda x: haversine((x[\"위도\"], x[\"경도\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "    within_data = trading_area.loc[dist_list <= dist]\n",
    "    \n",
    "    \n",
    "    for i, ctgr in enumerate(category_list):\n",
    "        data_copy[f\"store_category_{i}\"] = (within_data[\"상권업종중분류명\"] == ctgr).sum()\n",
    "\n",
    "    return data_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "colored-translation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:09<00:00,  1.16it/s]\n",
      " 45%|████▌     | 5/11 [00:03<00:05,  1.20it/s]]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.10it/s]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.08it/s]\n",
      " 45%|████▌     | 5/11 [00:03<00:05,  1.19it/s]]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.05it/s]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.06it/s]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.07it/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.10it/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.12it/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.16it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.7 s, sys: 589 ms, total: 19.3 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bus_stop_401_info = parallelize_dataframe(df = bus_stop_401_info, \n",
    "                                           func = count_store_nearby, \n",
    "                                           num_cores = 12, \n",
    "                                           trading_area = trading_area, \n",
    "                                           dist = 0.2, \n",
    "                                           category_list = category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-america",
   "metadata": {},
   "source": [
    "### 병원정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "tracked-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_data = pd.read_parquet(\"/home/seho/Passenger_Demand/data/hospital_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "familiar-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_data[\"category\"] = hospital_data[\"의료기관종별\"].replace({\"한방병원\" : \"병원\",\n",
    "                                                                  \"치과병원\" : \"병원\",\n",
    "                                                                  \"일반요양병원\" : \"요양병원\",\n",
    "                                                                  \"부속의원\" : \"의원\",\n",
    "                                                                  \"치과의원\" : \"의원\",\n",
    "                                                                  \"한의원\" : \"의원\",\n",
    "                                                                  \"보건지소\" : \"보건소\",\n",
    "                                                                  \"보건진료소\" : \"보건소\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "gorgeous-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_category_list = hospital_data[\"category\"].drop_duplicates().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "classical-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hospital_nearby(data, hospital_data = hospital_data, dist = 0.2, category_list = None):\n",
    "    data_copy = data.copy()\n",
    "    if category_list == None:\n",
    "        category_list = hospital_data[\"category\"].drop_duplicates().to_list()\n",
    "    \n",
    "    dist_list = hospital_data[[\"lat\", \"lng\"]].apply(lambda x: haversine((x[\"lat\"], x[\"lng\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "    within_data = hospital_data.loc[dist_list <= dist]\n",
    "    \n",
    "    \n",
    "    for i, ctgr in enumerate(category_list):\n",
    "        data_copy[f\"hospital_category_{i}\"] = (within_data[\"category\"] == ctgr).sum()\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "female-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 22.59it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 22.89it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 21.29it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 20.96it/s]\n",
      " 82%|████████▏ | 9/11 [00:00<00:00, 21.81it/s]]\n",
      "100%|██████████| 11/11 [00:00<00:00, 20.38it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 20.88it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 20.02it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 20.67it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 21.63it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 22.64it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 728 ms, sys: 503 ms, total: 1.23 s\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bus_stop_401_info = parallelize_dataframe(df = bus_stop_401_info, \n",
    "                                           func = count_hospital_nearby, \n",
    "                                           num_cores = 12, \n",
    "                                           hospital_data = hospital_data, \n",
    "                                           dist = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-moses",
   "metadata": {},
   "source": [
    "### 학교정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ordered-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data = pd.read_excel(\"/home/seho/Passenger_Demand/data/gv_school.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "stylish-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data[\"표준일차명\"] = school_data[\"표준일차명\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "liberal-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data = school_data.loc[school_data[\"표준일차명\"].str.contains(\"울산\", na=\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "appreciated-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "bacterial-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = googlemaps.Client(key='AIzaSyBRxjIW7qfFhaVyCsc2xhk5mf1hXUSi9DI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "proprietary-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geocode(x, gmaps):\n",
    "    try:\n",
    "        result = gmaps.geocode(x)[0][\"geometry\"][\"location\"]\n",
    "        # result = [temp[\"lat\"], temp[\"lng\"]]\n",
    "    except:\n",
    "        result = None\n",
    "    \n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "comprehensive-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data[\"category\"] = school_data[\"학교종류\"].replace({\"전문대학(3년제)\" : \"전문대학\",\n",
    "                                                          \"사내대학(전문)\" : \"전문대학\",\n",
    "                                                          \"기능대학\" : \"전문대학\",\n",
    "                                                          \"일반대학원\" : \"대학원\",\n",
    "                                                          \"전문대학원\" : \"대학원\",\n",
    "                                                          \"특수대학원\" : \"대학원\",\n",
    "                                                          \"일반고등학교\" : \"고등학교\",\n",
    "                                                          \"공업고등학교\" : \"고등학교\",\n",
    "                                                          \"상업고등학교\" : \"고등학교\",\n",
    "                                                          \"가사고등학교\" : \"고등학교\",\n",
    "                                                          \"체육고등학교\" : \"고등학교\",\n",
    "                                                          \"외국어고등학교\" : \"고등학교\",\n",
    "                                                          \"과학고등학교\" : \"고등학교\",\n",
    "                                                          \"예술고등학교\" : \"고등학교\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "upset-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "school_data[\"lat_lng\"] = school_data[\"새주소\"].apply(get_geocode, gmaps = gmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "national-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data[\"lat\"] = school_data[\"lat_lng\"].apply(lambda x: x[\"lat\"])\n",
    "school_data[\"lng\"] = school_data[\"lat_lng\"].apply(lambda x: x[\"lng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "hollow-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_school_nearby(data, school_data = school_data, dist = 0.2, category_list = None):\n",
    "    data_copy = data.copy()\n",
    "    if category_list == None:\n",
    "        category_list = school_data[\"category\"].drop_duplicates().to_list()\n",
    "    \n",
    "    dist_list = school_data[[\"lat\", \"lng\"]].apply(lambda x: haversine((x[\"lat\"], x[\"lng\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "    within_data = school_data.loc[dist_list <= dist]\n",
    "    \n",
    "    \n",
    "    for i, ctgr in enumerate(category_list):\n",
    "        data_copy[f\"school_category_{i}\"] = (within_data[\"category\"] == ctgr).sum()\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "understanding-nursing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 75.12it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 78.86it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 71.43it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 69.90it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 69.26it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 66.73it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 65.95it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 66.10it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 67.34it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 71.13it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 74.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 77.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 424 ms, sys: 449 ms, total: 872 ms\n",
      "Wall time: 975 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bus_stop_401_info = parallelize_dataframe(df = bus_stop_401_info, \n",
    "                                              func = count_school_nearby, \n",
    "                                              num_cores = 12, \n",
    "                                              school_data = school_data, \n",
    "                                              dist = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-spice",
   "metadata": {},
   "source": [
    "### 정류장 정보 Join(거리기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "alone-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(ml_data, bus_stop_401_info.drop([\"stop_id\", \"city\"],1), on = \"mybi_stop_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "explicit-aruba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 175)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-xerox",
   "metadata": {},
   "source": [
    "### 울산행사정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "generous-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = pd.read_csv(\"~/Passenger_Demand/data/ulsan_event_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "baking-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data[\"eventStartDate\"] = pd.to_datetime(event_data[\"eventStartDate\"], format = \"%Y-%m-%d\")\n",
    "event_data[\"eventEndDate\"] = pd.to_datetime(event_data[\"eventEndDate\"], format = \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "english-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_event_nearby(data, event_data, dist = 0.2):\n",
    "    data_copy = data.copy()\n",
    "    within_data = event_data.loc[(event_data[\"eventStartDate\"] <= data_copy[\"transdate\"]) & (event_data[\"eventEndDate\"] >= data_copy[\"transdate\"])]\n",
    "    \n",
    "    if len(within_data) == 0:\n",
    "        data_copy[f\"event_nearby\"] = 0\n",
    "    else:\n",
    "        dist_list = within_data[[\"latitude\", \"longitude\"]].apply(lambda x: haversine((x[\"latitude\"], x[\"longitude\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "        data_copy[f\"event_nearby\"] = (dist_list <= dist).sum()\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "protected-graduate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51745/51745 [04:27<00:00, 193.52it/s]\n",
      "100%|██████████| 51745/51745 [04:26<00:00, 194.04it/s]\n",
      "100%|██████████| 51745/51745 [04:28<00:00, 192.79it/s]\n",
      "100%|██████████| 51745/51745 [04:29<00:00, 192.17it/s]\n",
      "100%|██████████| 51745/51745 [04:24<00:00, 195.47it/s]\n",
      "100%|██████████| 51745/51745 [04:26<00:00, 194.07it/s]\n",
      "100%|██████████| 51745/51745 [04:26<00:00, 194.53it/s]\n",
      "100%|██████████| 51745/51745 [04:27<00:00, 193.25it/s]\n",
      "100%|██████████| 51745/51745 [04:24<00:00, 195.72it/s]\n",
      "100%|██████████| 51745/51745 [04:24<00:00, 195.89it/s]\n",
      "100%|██████████| 51745/51745 [04:26<00:00, 193.81it/s]\n",
      "100%|██████████| 51745/51745 [04:25<00:00, 195.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.9 s, sys: 18.8 s, total: 48.7 s\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ml_data = parallelize_dataframe(df = ml_data, \n",
    "                                func = count_event_nearby, \n",
    "                                num_cores = 12, \n",
    "                                event_data = event_data, \n",
    "                                dist = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "innovative-teacher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 176)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-capture",
   "metadata": {},
   "source": [
    "### 축제 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "numerical-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "festival_data = pd.read_csv(\"~/Passenger_Demand/data/festival_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "fatty-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "festival_data[\"fstvlStartDate\"] = pd.to_datetime(festival_data[\"fstvlStartDate\"], format = \"%Y-%m-%d\")\n",
    "festival_data[\"fstvlEndDate\"] = pd.to_datetime(festival_data[\"fstvlEndDate\"], format = \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "burning-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_festival_nearby(data, festival_data, dist = 0.2):\n",
    "    data_copy = data.copy()\n",
    "    within_data = festival_data.loc[(festival_data[\"fstvlStartDate\"] <= data_copy[\"transdate\"]) & (festival_data[\"fstvlEndDate\"] >= data_copy[\"transdate\"])]\n",
    "    \n",
    "    if len(within_data) == 0:\n",
    "        data_copy[f\"festival_nearby\"] = 0\n",
    "    else:\n",
    "        dist_list = within_data[[\"latitude\", \"longitude\"]].apply(lambda x: haversine((x[\"latitude\"], x[\"longitude\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "        data_copy[f\"festival_nearby\"] = (dist_list <= dist).sum()\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "caring-carrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51745/51745 [01:53<00:00, 455.71it/s]\n",
      "100%|█████████▉| 51576/51745 [01:45<00:00, 309.50it/s]\n",
      "100%|██████████| 51745/51745 [01:54<00:00, 452.62it/s]\n",
      "100%|██████████| 51745/51745 [01:53<00:00, 455.10it/s]\n",
      "100%|██████████| 51745/51745 [01:53<00:00, 457.21it/s]\n",
      "100%|██████████| 51745/51745 [01:53<00:00, 455.10it/s]\n",
      "100%|██████████| 51745/51745 [01:53<00:00, 456.65it/s]\n",
      "100%|██████████| 51745/51745 [01:52<00:00, 459.80it/s]\n",
      "100%|██████████| 51745/51745 [01:53<00:00, 454.17it/s]\n",
      "100%|██████████| 51745/51745 [01:51<00:00, 462.20it/s]\n",
      "100%|██████████| 51745/51745 [01:52<00:00, 458.51it/s]\n",
      "100%|██████████| 51745/51745 [01:51<00:00, 462.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.1 s, sys: 10.5 s, total: 28.6 s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ml_data = parallelize_dataframe(df = ml_data, \n",
    "                                func = count_festival_nearby, \n",
    "                                num_cores = 12, \n",
    "                                festival_data = festival_data, \n",
    "                                dist = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "sophisticated-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 177)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-respondent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "arranged-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data.to_pickle(\"/home/seho/Passenger_Demand/data/ml_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "registered-retention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transdate', 'mybi_stop_id', 'normalcnt', 'studentcnt', 'childcnt',\n",
       "       'totalcnt', 'dayofweek', 'hour', 'totalcnt_bf1d', 'totalcnt_bf2d',\n",
       "       ...\n",
       "       'hospital_category_4', 'hospital_category_5', 'school_category_0',\n",
       "       'school_category_1', 'school_category_2', 'school_category_3',\n",
       "       'school_category_4', 'school_category_5', 'event_nearby',\n",
       "       'festival_nearby'],\n",
       "      dtype='object', length=177)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-tribe",
   "metadata": {},
   "source": [
    "### 인구 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "protected-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_data = pd.read_csv(\"~/Passenger_Demand/data/울산광역시_인구 현황_20200727.csv\", encoding = \"euc-kr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-mauritius",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
