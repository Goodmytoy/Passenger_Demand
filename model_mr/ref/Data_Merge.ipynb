{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bigger-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blind-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Parallelize_DataFrame import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "valued-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-bailey",
   "metadata": {},
   "source": [
    "### 마이비 카드 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "humanitarian-footage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 5.48 s, total: 27.8 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mybicard = pd.read_parquet('/home/seho/Passenger_Demand/data/mybicard.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "personal-mouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36261767, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybicard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "injured-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수집일자 데이트 포맷으로 변환\n",
    "#mybicard[\"collectdate\"] = pd.to_datetime(mybicard[\"collectdate\"], format = \"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "listed-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전송일자 데이트 포맷으로 변환\n",
    "mybicard[\"transdate\"] = pd.to_datetime(mybicard[\"transdate\"], format = \"%Y%m%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "changed-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 324 ms, sys: 349 ms, total: 673 ms\n",
      "Wall time: 122 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mybicard[\"totalcnt\"] = mybicard[\"normalcnt\"] + mybicard[\"studentcnt\"] + mybicard[\"childcnt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "contemporary-ground",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 178 ms, sys: 150 ms, total: 327 ms\n",
      "Wall time: 325 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 전체 승객 수 변수 생성(일반 + 학생 + 아동)\n",
    "mybicard[\"totalcnt\"] = mybicard[[\"normalcnt\", \"studentcnt\", \"childcnt\"]].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "designed-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_nm에 공백이 포함되어 있어 공백 제거\n",
    "mybicard[\"route_nm\"] = mybicard[\"route_nm\"].replace(\"\\s\", \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dangerous-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ;mybicard = mybicard.sort_values([\"transdate\", \"seq\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "collect-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard = mybicard.rename(columns = {\"stop_id\" : \"mybi_stop_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-tamil",
   "metadata": {},
   "source": [
    "### 401번 버스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ignored-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401 = mybicard.loc[(mybicard[\"route_nm\"] == \"401\") & (mybicard[\"transflag\"].isin([\"환승\", \"비환승\"])), [\"route_nm\", \"transdate\", \"mybi_stop_id\", \"normalcnt\", \"studentcnt\", \"childcnt\", \"totalcnt\"]].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unavailable-estimate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1964401, 17)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybicard_401.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "least-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1770869, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybicard_401.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-clearance",
   "metadata": {},
   "source": [
    "### 정류장 X,Y 좌표 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "stopped-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경주시, 양산시, 울산광역시, 부산광역시\n",
    "bus_stop_info = pd.read_csv(\"/home/seho/Passenger_Demand/data/울산광역시_버스 정류소 위치 정보_20200531.csv\", encoding = \"euc-kr\")\n",
    "bus_stop_info = bus_stop_info.loc[bus_stop_info[\"권역\"] == \"울산광역시\"]\n",
    "bus_stop_info.columns = [\"stop_nm\", \"stop_id\", \"longitude\", \"latitude\", \"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "noble-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_401_1 = pd.read_csv(\"/home/seho/Passenger_Demand/data/401_율리_꽃바위.csv\", encoding = \"euc_kr\")\n",
    "bus_stop_401_2 = pd.read_csv(\"/home/seho/Passenger_Demand/data/401_꽃바위_율리.csv\", encoding = \"euc_kr\")\n",
    "bus_stop_401 = pd.concat([bus_stop_401_1, bus_stop_401_2])\n",
    "bus_stop_401.columns = [\"mybi_stop_id\", \"stop_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "serial-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_401_info = pd.merge(bus_stop_401, bus_stop_info, on = \"stop_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "loaded-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401 = pd.merge(mybicard_401, bus_stop_401_info[[\"mybi_stop_id\", \"stop_id\", \"stop_nm\", \"longitude\", \"latitude\"]], on = \"mybi_stop_id\", how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "authorized-microwave",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mybicard_401 = mybicard_401.drop([\"mybi_stop_id\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "saved-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401 = mybicard_401.loc[mybicard_401[\"transdate\"].dt.hour.isin([1,2,3,4]) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "familiar-history",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1770869, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybicard_401.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "psychological-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_info = mybicard_401[[\"stop_id\", \"stop_nm\", \"longitude\", \"latitude\"]].drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "above-speech",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_nm</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24234</td>\n",
       "      <td>일산해수욕장</td>\n",
       "      <td>129.428013</td>\n",
       "      <td>35.497744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40416</td>\n",
       "      <td>목화예식장앞</td>\n",
       "      <td>129.330419</td>\n",
       "      <td>35.538175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25005</td>\n",
       "      <td>우성현대아파트</td>\n",
       "      <td>129.418852</td>\n",
       "      <td>35.483501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25007</td>\n",
       "      <td>방어동 행정복지센터</td>\n",
       "      <td>129.424330</td>\n",
       "      <td>35.485841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24219</td>\n",
       "      <td>현대중공업 울산대학병원</td>\n",
       "      <td>129.432616</td>\n",
       "      <td>35.522595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>22402</td>\n",
       "      <td>옥현주공아파트앞</td>\n",
       "      <td>129.264414</td>\n",
       "      <td>35.545098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>40620</td>\n",
       "      <td>보건환경연구원</td>\n",
       "      <td>129.270913</td>\n",
       "      <td>35.538592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>30597</td>\n",
       "      <td>문수실버복지관</td>\n",
       "      <td>129.251570</td>\n",
       "      <td>35.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>24204</td>\n",
       "      <td>정수장</td>\n",
       "      <td>129.409310</td>\n",
       "      <td>35.530381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>30503</td>\n",
       "      <td>율리공영차고지종점</td>\n",
       "      <td>129.246955</td>\n",
       "      <td>35.529615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     stop_id       stop_nm   longitude   latitude\n",
       "0      24234        일산해수욕장  129.428013  35.497744\n",
       "1      40416        목화예식장앞  129.330419  35.538175\n",
       "2      25005       우성현대아파트  129.418852  35.483501\n",
       "3      25007    방어동 행정복지센터  129.424330  35.485841\n",
       "4      24219  현대중공업 울산대학병원  129.432616  35.522595\n",
       "..       ...           ...         ...        ...\n",
       "126    22402      옥현주공아파트앞  129.264414  35.545098\n",
       "127    40620       보건환경연구원  129.270913  35.538592\n",
       "128    30597       문수실버복지관  129.251570  35.534900\n",
       "129    24204           정수장  129.409310  35.530381\n",
       "130    30503     율리공영차고지종점  129.246955  35.529615\n",
       "\n",
       "[131 rows x 4 columns]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_stop_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-maker",
   "metadata": {},
   "source": [
    "### 결측치 \n",
    "하루의 수집 데이터의 수가 0인 날짜의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "compact-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_agg(data, date_col, stop_id_col, target_cols, freq = None, groupby_cols = None, agg_dict = None, agg_func = sum):\n",
    "    \n",
    "    if isinstance(target_cols, list) == False:\n",
    "        target_cols = [target_cols]\n",
    "        \n",
    "    if isinstance(groupby_cols, list) == False:\n",
    "        if groupby_cols is None:\n",
    "            groupby_cols = []\n",
    "        else:\n",
    "            groupby_cols = [groupby_cols]\n",
    "    \n",
    "    grouper = pd.Grouper(key = date_col, freq = freq)\n",
    "    \n",
    "    # 1시간 단위로 Target 변수들을 집계\n",
    "    if agg_dict is None:\n",
    "        agg_dict = {col : agg_func for col in target_cols}\n",
    "    \n",
    "    groupby_cols.append(stop_id_col)\n",
    "    if freq is not None:\n",
    "        groupby_cols.append(grouper)\n",
    "\n",
    "    data_agg = (data.groupby(groupby_cols)\n",
    "                    .agg(agg_dict)\n",
    "                    .reset_index())\n",
    "    \n",
    "    return data_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "cheap-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = create_data_agg(mybicard_401, date_col = \"transdate\", stop_id_col = \"stop_id\", target_cols = [\"totalcnt\", \"normalcnt\", \"studentcnt\", \"childcnt\"], freq = \"60min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "synthetic-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(data, date_col):\n",
    "    \n",
    "    # 시간 변수들 생성\n",
    "    # 요일\n",
    "    data[\"dayofweek\"] = data[date_col].dt.dayofweek\n",
    "    dow_dict = {0:\"월\", 1:\"화\", 2:\"수\", 3:\"목\", 4:\"금\", 5:\"토\", 6:\"일\"}\n",
    "    data[\"dayofweek\"] = data[\"dayofweek\"].replace(dow_dict)\n",
    "    # 시간\n",
    "    data[\"hour\"] = data[date_col].dt.hour\n",
    "    # 일\n",
    "    data[\"date\"] = data[\"transdate\"].dt.date   \n",
    "    # 월\n",
    "    data[\"month\"] = data[date_col].dt.month\n",
    "    # 주\n",
    "    data[\"weekofyear\"] = data[date_col].dt.isocalendar().week\n",
    "  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "fluid-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_date(data, date_col, stop_id_col, except_hours = None):\n",
    "    \n",
    "    if isinstance(except_hours, list) == False:\n",
    "        except_hours = [except_hours]\n",
    "        \n",
    "    # 정류장별 모든 시간대의 조합을 생성해 버스 집계 데이터를 Join\n",
    "    # 데이터가 존재하지 않는 시간대 : NA -> 이후 Impute\n",
    "    \n",
    "    # 데이터의 시작과 끝 사이를 1시간 간격으로 구분하여 list 생성\n",
    "    dt_list = pd.date_range(start = data[date_col].min(), end = data[date_col].max(), freq = \"1h\")\n",
    "    date_df = pd.DataFrame({date_col : dt_list}).reset_index(drop = True)\n",
    "    stop_id_df = pd.DataFrame({stop_id_col : data[stop_id_col].drop_duplicates()}).reset_index(drop = True)\n",
    "\n",
    "    # 전체 일정(시간 단위)과 정류소 별 조합 DF 생성\n",
    "    all_date = pd.merge(date_df, stop_id_df, how = \"cross\")\n",
    "    \n",
    "    # 결측일의 데이터를 채워넣은 전체 데이터를 left join\n",
    "    all_date = pd.merge(all_date, data, on = [date_col, stop_id_col], how = \"left\")\n",
    "    \n",
    "    all_date = all_date.loc[all_date[date_col].dt.hour.isin(except_hours) == False]\n",
    "    \n",
    "    return all_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "coral-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_date = create_all_date(data = base_data, date_col = \"transdate\", stop_id_col = \"stop_id\", except_hours=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "boxed-delhi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563300, 6)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "labeled-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_date = add_time_features(data = all_date, date_col = \"transdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "wired-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n주 전 같은 요일 같은 시간대의 인원 수로 Impute\n",
    "def impute_recent_data(data, missing_date, date_col = \"transdate\"):\n",
    "    data = data.copy()\n",
    "    for x in missing_date:\n",
    "        temp = []\n",
    "        w = 0\n",
    "        while len(temp) == 0:\n",
    "            w +=1\n",
    "            temp = data.loc[data[date_col].dt.date == (x - timedelta(weeks = w)).date()].copy()\n",
    "\n",
    "        temp[date_col] = temp[date_col] + timedelta(weeks = w)\n",
    "        data = pd.concat([data, temp], 0)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "attended-deviation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 최근 n주의 같은 요일 같은 시간대의 평균값으로 Impute\n",
    "def impute_recent_mean_data(data, missing_date, date_col):\n",
    "\n",
    "    data = data.copy()\n",
    "\n",
    "    # 요일, 시간 추가\n",
    "    data[\"dayofweek\"] = data[\"transdate\"].dt.dayofweek\n",
    "    dow_dict = {0:\"월\", 1:\"화\", 2:\"수\", 3:\"목\", 4:\"금\", 5:\"토\", 6:\"일\"}\n",
    "    data[\"dayofweek\"] = data[\"dayofweek\"].replace(dow_dict)\n",
    "    data[\"hour\"] = data[\"transdate\"].dt.hour\n",
    "         \n",
    "    \n",
    "    for x in missing_date:\n",
    "        base_date = x\n",
    "        w = 0\n",
    "        # 결측일의 이전 4주를 기본으로 검색하며, 데이터가 없는 경우 범위를 1주씩 늘려가며 데이터 조회\n",
    "        temp = []\n",
    "        while len(temp) == 0:\n",
    "            temp = data.loc[(data[\"transdate\"].dt.date.between((x - timedelta(weeks = 4+w)).date(), x.date())) & (data[\"transdate\"].dt.dayofweek == x.day_of_week)].copy()\n",
    "            w += 1\n",
    "\n",
    "        # 4+w 전까지의 데이터를 찾아서 정류장별, 요일별, 시간별 평균값 산출\n",
    "        temp2 = temp.groupby([\"mybi_stop_id\", \"dayofweek\", \"hour\"]).agg({\"totalcnt\" : np.mean,\n",
    "                                                                         \"normalcnt\" : np.mean,\n",
    "                                                                         \"studentcnt\" : np.mean,\n",
    "                                                                         \"childcnt\" : np.mean}).reset_index()\n",
    "        # 평균값 변환 (Float -> Int : 반올림 효과)\n",
    "        temp2[\"totalcnt\"] = temp2[\"totalcnt\"].astype(int)\n",
    "        temp2[\"normalcnt\"] = temp2[\"normalcnt\"].astype(int)\n",
    "        temp2[\"studentcnt\"] = temp2[\"studentcnt\"].astype(int)\n",
    "        temp2[\"childcnt\"] = temp2[\"childcnt\"].astype(int)\n",
    "\n",
    "        # 기준 일자, 시간으로 부터 transdate을 재생성\n",
    "        temp2[\"transdate\"] = temp2.apply(lambda x: base_date + timedelta(hours = x[\"hour\"]), 1)\n",
    "\n",
    "        data = pd.concat([data, temp2], 0)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "worse-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_bus_demand_data(data, date_col, stop_id_col):\n",
    "    # 일 단위 집계 -> 데이터가 존재하지 않는 일은 결측일로 판단 (missing_date)\n",
    "    count_by_date = data.groupby([pd.Grouper(key=date_col, freq=\"1D\")]).size().reset_index(name = \"cnt\")\n",
    "    missing_date = count_by_date.loc[count_by_date[\"cnt\"] == 0, date_col]\n",
    "    \n",
    "    # 1) 결측일을 제외한 결측치(데이터가 존재하지 않는 시간대)는 승객이 0명 이므로 0으로 대체\n",
    "    data = data.loc[data[date_col].dt.date.isin(missing_date.dt.date) == False].fillna(0)\n",
    "    \n",
    "    # 2) 최근 n주의 같은 요일 같은 시간대의 평균값으로 Impute\n",
    "    data = impute_recent_mean_data(data = data, missing_date = missing_date, date_col = \"transdate\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "controlled-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_date = impute_bus_demand_data(data = all_date, date_col = \"transdate\", stop_id_col = \"stop_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-recommendation",
   "metadata": {},
   "source": [
    "### 시계열 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "bound-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_feature(data, target_cols, date_cols, lags, type = \"hour\", groupby_cols = None):\n",
    "    data = data.copy()\n",
    "    if isinstance(lags, list) == False:\n",
    "        lags = [lags]\n",
    "    if isinstance(date_cols, list) == False:\n",
    "        date_cols = [date_cols]\n",
    "    if isinstance(target_cols, list) == False:\n",
    "        target_cols = [target_cols]\n",
    "    if isinstance(groupby_cols, list) == False:\n",
    "        groupby_cols = [groupby_cols]\n",
    "                \n",
    "    \n",
    "    for lg in lags:\n",
    "        if groupby_cols is None:\n",
    "            cnt_bf = data.set_index(date_cols)[target_cols].shift(freq = lg).reset_index()\n",
    "        else:\n",
    "            cnt_bf = data.set_index(date_cols).groupby(groupby_cols)[target_cols].shift(freq = lg).reset_index()\n",
    "        \n",
    "        rename_dict = {col: f\"{col}_bf_{lg}\" for col in target_cols}\n",
    "        cnt_bf = cnt_bf.rename(columns = rename_dict)\n",
    "        \n",
    "        data = pd.merge(data, cnt_bf, on = date_cols + groupby_cols, how = \"left\")\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "accomplished-hazard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 29.5 ms, total: 1.86 s\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_date = create_lag_feature(data = all_date, target_cols = [\"totalcnt\", \"normalcnt\", \"studentcnt\", \"childcnt\"], date_cols = \"transdate\", lags = [\"1d\", \"2d\", \"3d\", \"4d\", \"5d\", \"6d\", \"7d\"], groupby_cols = \"stop_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "familiar-infection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563300, 39)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date.shape\n",
    "# (563300, 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-possible",
   "metadata": {},
   "source": [
    "### 날짜별 평균 Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "ordered-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg_daily_mean = create_data_agg(data = all_date, date_col = \"transdate\", stop_id_col = \"stop_id\", target_cols = [\"totalcnt\", \"normalcnt\", \"studentcnt\", \"childcnt\"], freq = \"1D\", agg_func = np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "renewable-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [\"1d\", \"2d\", \"3d\", \"4d\", \"5d\", \"6d\", \"7d\"]\n",
    "rename_dict = {f\"{col}_bf_{lg}\": f\"{col}_bf_{lg}_total\" for col in [\"totalcnt\"] for lg in lags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "mighty-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_lag = create_lag_feature(data = data_agg_daily_mean, target_cols = \"totalcnt\", date_cols = \"transdate\", lags = lags, groupby_cols = \"stop_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "compliant-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_lag = daily_lag.rename(columns = rename_dict)\n",
    "daily_lag[\"date\"] = daily_lag[\"transdate\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "committed-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_date = pd.merge(all_date, daily_lag[[\"date\", \"stop_id\"] + list(rename_dict.values())], on = [\"date\", \"stop_id\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "continuous-mitchell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563300, 46)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-graph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "alpine-charity",
   "metadata": {},
   "source": [
    "### Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-reservation",
   "metadata": {},
   "source": [
    "#### 1) 이전 n개일자들의 동일 시간대 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "boxed-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_agg(data, target_cols, date_col, groupby_cols, col_nm = \"\", rollings = [\"2d\"], agg_func = [np.mean, np.std]):\n",
    "    if isinstance(target_cols, list) == False:\n",
    "        target_cols = [target_cols]\n",
    "        \n",
    "    if isinstance(groupby_cols, list) == False:\n",
    "        groupby_cols = [groupby_cols]\n",
    "        \n",
    "    if col_nm != \"\":\n",
    "        col_nm = f\"{col_nm}_\"\n",
    "    \n",
    "    \n",
    "    for rl in rollings:\n",
    "        for tg in target_cols:\n",
    "            data = data.set_index(date_col).sort_index(ascending=True).copy()\n",
    "            rolling_data = data.groupby(groupby_cols)[tg].rolling(rl).agg(agg_func)\n",
    "            rolling_data = rolling_data.rename(columns = {\"mean\" : f\"{tg}_ma_{col_nm}mean_{rl}\", \n",
    "                                                          \"std\" : f\"{tg}_ma_{col_nm}std_{rl}\"})\n",
    "            rolling_data = rolling_data.groupby(groupby_cols).shift(1).reset_index()    \n",
    "            \n",
    "            data = pd.merge(data.reset_index(), rolling_data, on = [date_col] + groupby_cols, how = \"left\")\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "final-shelter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.22 s, sys: 0 ns, total: 2.22 s\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_date = calculate_moving_agg(data = all_date, target_cols = [\"totalcnt\"], date_col = \"transdate\", groupby_cols = [\"stop_id\", \"hour\"], col_nm = \"hour\", rollings = [\"2d\", \"3d\", \"4d\", \"5d\", \"6d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "south-construction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563300, 56)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date.shape\n",
    "# (620940, 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-antenna",
   "metadata": {},
   "source": [
    "#### 2) n주전까지의 동일 요일의 동일 시간대 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "encouraging-aurora",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.13 s, sys: 8.15 ms, total: 3.13 s\n",
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_date = calculate_moving_agg(data = all_date, target_cols = [\"totalcnt\"], date_col = \"transdate\", groupby_cols = [\"stop_id\", \"hour\", \"dayofweek\"], col_nm = \"hour_week\", rollings = [\"14d\", \"21d\", \"28d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "alleged-fitness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563300, 62)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date.shape\n",
    "# (620940, 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-district",
   "metadata": {},
   "source": [
    "#### 3) 이전 n개일자들의 전체 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "revised-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg_daily_sum = create_data_agg(mybicard_401, date_col = \"transdate\", stop_id_col = \"stop_id\", target_cols = [\"totalcnt\"], freq = \"1D\", agg_func = sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "remarkable-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg_daily_sum = add_time_features(data_agg_daily_sum, date_col = \"transdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "adjusted-baker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 101 ms, sys: 97 µs, total: 101 ms\n",
      "Wall time: 101 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daily_mv_agg = calculate_moving_agg(data = data_agg_daily_sum, target_cols = [\"totalcnt\"], date_col = \"transdate\", groupby_cols = [\"stop_id\"], col_nm = \"daily\", rollings = [\"2d\", \"3d\", \"4d\", \"5d\", \"6d\"])\n",
    "daily_mv_agg[\"date\"] = daily_mv_agg[\"transdate\"].dt.date\n",
    "daily_mv_agg = daily_mv_agg.drop([\"transdate\", \"totalcnt\", \"dayofweek\", \"hour\", \"month\", \"weekofyear\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "martial-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_date = pd.merge(all_date, daily_mv_agg, on = [\"stop_id\", \"date\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "taken-spare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563300, 72)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date.shape\n",
    "# (563300, 72)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-assignment",
   "metadata": {},
   "source": [
    "#### 4) n주전까지의 동일 요일의 전체 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "molecular-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 146 ms, sys: 0 ns, total: 146 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daily_week_mv_agg = calculate_moving_agg(data = data_agg_daily_sum, target_cols = [\"totalcnt\"], date_col = \"transdate\", groupby_cols = [\"stop_id\", \"dayofweek\"], col_nm = \"daily_week\", rollings = [\"14d\", \"21d\", \"28d\"])\n",
    "daily_week_mv_agg[\"date\"] = daily_week_mv_agg[\"transdate\"].dt.date\n",
    "daily_week_mv_agg = daily_week_mv_agg.drop([\"transdate\",\"totalcnt\", \"dayofweek\", \"hour\", \"month\", \"weekofyear\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "scheduled-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_date = pd.merge(all_date, daily_week_mv_agg, on = [\"stop_id\", \"date\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "dense-mixture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563300, 78)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date.shape\n",
    "# (620940, 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-elevation",
   "metadata": {},
   "source": [
    "#### 5) n주전까지의 주 평균의 이동평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "unusual-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "mybicard_401 = add_time_features(mybicard_401, date_col = \"transdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "specified-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg_weekly_mean = create_data_agg(data_agg_daily_sum, date_col = \"transdate\", stop_id_col = \"stop_id\", groupby_cols = \"weekofyear\",  target_cols = [\"totalcnt\"], agg_func = np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "lonely-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_mv_agg = calculate_moving_agg(data = data_agg_weekly_mean, target_cols = [\"totalcnt\"], date_col = \"weekofyear\", groupby_cols = \"stop_id\", col_nm = \"weekly\", rollings = [2,3,4])\n",
    "weekly_mv_agg = weekly_mv_agg.drop(\"totalcnt\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "stretch-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_date = pd.merge(all_date, weekly_mv_agg, on = [\"stop_id\", \"weekofyear\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "small-demonstration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563300, 84)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date.shape\n",
    "# (620940, 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-document",
   "metadata": {},
   "source": [
    "### 특일 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "spiritual-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_data = pd.read_parquet(\"/home/seho/Passenger_Demand/data/holiday_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "joint-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seq_y(data, criterion = 2):\n",
    "    seq_list = []\n",
    "    Y_cnt = 0\n",
    "    for i, x in enumerate(data):\n",
    "        if x == \"Y\":\n",
    "            Y_cnt += 1\n",
    "\n",
    "        if (x == \"N\") | (i == len(data)):\n",
    "            if Y_cnt > criterion:\n",
    "                temp_list = [\"Y\"] * Y_cnt\n",
    "                seq_list += temp_list\n",
    "            elif (Y_cnt > 0) & (Y_cnt <= criterion):\n",
    "                temp_list = [\"N\"] * Y_cnt\n",
    "                seq_list += temp_list\n",
    "            seq_list.append(\"N\")\n",
    "            Y_cnt = 0\n",
    "            \n",
    "    return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "designed-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_holiday_data(holiday_data, \n",
    "                            date_col = \"locdate\"):\n",
    "    \"\"\"\n",
    "        특일 데이터를 전처리 하는 함수\n",
    "        \n",
    "        Args: \n",
    "            holiday_data: 특일 데이터 (Pandas.DataFrame)\n",
    "            date_col: 날짜 컬럼명 (str)\n",
    "\n",
    "        Returns: \n",
    "            data: lag feature를 생성한 데이터 (Pandas.DataFrame)\n",
    "\n",
    "        Exception: \n",
    "            \n",
    "    \"\"\"    \n",
    "    holiday_data[\"date\"] = pd.to_datetime(holiday_data[date_col], format = \"%Y%m%d\")\n",
    "    \n",
    "    # 전체 일자 생성\n",
    "    start_year = holiday_data[\"date\"].dt.year.min()\n",
    "    end_year = holiday_data[\"date\"].dt.year.max()\n",
    "    \n",
    "    date_df = pd.DataFrame({\"date\" : pd.date_range(f\"{start_year}-01-01\", f\"{end_year}-12-31\", freq = \"1D\")})\n",
    "#     date_df[\"date\"] = date_df[\"date\"].dt.date\n",
    "    \n",
    "    # 1) 주말 여부\n",
    "    # 주말 여부를 Y/N으로 표시\n",
    "    date_df[\"weekend\"] = np.where(date_df[\"date\"].dt.dayofweek.isin([5,6]), \"Y\", \"N\")    \n",
    "    \n",
    "    # 2) 공휴일, 명절 여부\n",
    "    # 명절(ntl_holiday)\n",
    "    ntl_holiday = holiday_data.loc[holiday_data[\"dateName\"].isin([\"설날\", \"추석\"])]\n",
    "    ntl_holiday = ntl_holiday.rename(columns = {\"dateName\" : \"ntl_holi\"})\n",
    "    # 공휴일(holiday) \n",
    "    holiday = holiday_data.loc[holiday_data[\"dateName\"].isin([\"설날\", \"추석\"]) == False]\n",
    "    holiday = holiday.rename(columns = {\"dateName\" : \"holi\"})\n",
    "    \n",
    "    # 곻휴일, 명절 여부 추가 (left join)\n",
    "    date_df = pd.merge(date_df, ntl_holiday.drop(\"locdate\", 1), on = \"date\", how = \"left\")\n",
    "    date_df = pd.merge(date_df, holiday.drop(\"locdate\", 1), on = \"date\", how = \"left\")\n",
    "    date_df[\"ntl_holi\"] = np.where(date_df[\"ntl_holi\"].isna(),\"N\", \"Y\")\n",
    "    date_df[\"holi\"] = np.where(date_df[\"holi\"].isna(),\"N\", \"Y\")\n",
    "    \n",
    "    # 3) 3일 이상 연휴 여부\n",
    "    date_df[\"rest_yn\"] = date_df[[\"weekend\", \"ntl_holi\", \"holi\"]].apply(lambda x: any(x == \"Y\"), 1)\n",
    "    date_df[\"rest_yn\"] = np.where(date_df[\"rest_yn\"],\"Y\", \"N\")\n",
    "    \n",
    "#     return date_df\n",
    "#     print(date_df.head())\n",
    "    date_df[\"seq_holi\"] = find_seq_y(data = date_df[\"rest_yn\"], criterion = 2)\n",
    "    date_df = date_df.drop([\"weekend\", \"rest_yn\"], 1)\n",
    "    date_df[\"date\"] = date_df[\"date\"].dt.date\n",
    "    \n",
    "    return date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "informational-baseball",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "holiday_data = preprocess_holiday_data(holiday_data = holiday_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "modified-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(all_date, holiday_data, on = \"date\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-impact",
   "metadata": {},
   "source": [
    "### 날씨 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "acute-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_parquet(\"/home/seho/Passenger_Demand/data/weather_2018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "retired-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(weather_data):\n",
    "    \"\"\"\n",
    "        날씨 데이터를 전처리 하는 함수\n",
    "        \n",
    "        Args: \n",
    "            weather_data: 날씨 데이터 (Pandas.DataFrame)\n",
    "\n",
    "        Returns: \n",
    "            data: 날씨 데이터를 전처리한 데이터 (Pandas.DataFrame)\n",
    "\n",
    "        Exception: \n",
    "            \n",
    "    \"\"\"     \n",
    "    # 필요 컬럼만 추출\n",
    "    # tm(시간), ta(기온), hm(습도), rn(강수량), dsnw(적설량)\n",
    "    weather_data = weather_data.loc[:, [\"tm\", \"ta\", \"hm\", \"rn\", \"dsnw\"]]\n",
    "    weather_data = weather_data.rename(columns = {\"tm\" : \"time\",\n",
    "                                                  \"ta\" : \"temperature\",\n",
    "                                                  \"hm\" : \"humidity\",\n",
    "                                                  \"rn\" : \"precipitation\",\n",
    "                                                  \"dsnw\" : \"snowfall\",})\n",
    "    weather_data[\"time\"] = pd.to_datetime(weather_data[\"time\"], format = \"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "    for col in [\"temperature\", \"humidity\", \"precipitation\", \"snowfall\"]:\n",
    "        weather_data[col] = weather_data[col].replace(\"\", \"0.0\").astype(float)\n",
    "        weather_data[col] = weather_data[col].astype(float)\n",
    "        \n",
    "    weather_data[\"time_hours\"] = weather_data[\"time\"].dt.strftime(\"%Y-%m-%d %H\")\n",
    "    weather_data = weather_data.drop(\"time\", 1)\n",
    "    \n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "developed-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = preprocess_weather_data(weather_data = weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-subscriber",
   "metadata": {},
   "source": [
    "### 미세먼지 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "loose-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data = pd.read_csv(\"/home/seho/Passenger_Demand/data/pm_data.csv\")\n",
    "# pm_data[\"issueDate\"] = pd.to_datetime(pm_data[\"issueDate\"], format = \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "conceptual-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pm_data(pm_data, date_col = \"issueDate\", city = \"울산\"):\n",
    "    \"\"\"\n",
    "        미세먼지 경보 데이터를 전처리 하는 함수\n",
    "        \n",
    "        Args: \n",
    "            pm_data: 날씨 데이터 (Pandas.DataFrame)\n",
    "            date_col: date_col: 날짜 컬럼명 (str)\n",
    "            city: 도시명 (str)\n",
    "            \n",
    "        Returns: \n",
    "            data: 미세먼지 경보 데이터를 전처리한 데이터 (Pandas.DataFrame)\n",
    "\n",
    "        Exception: \n",
    "            \n",
    "    \"\"\"\n",
    "    pm_data[date_col] = pd.to_datetime(pm_data[date_col], format = \"%Y-%m-%d\")\n",
    "    pm_data_agg = pm_data.loc[pm_data[\"districtName\"] == city].groupby(pd.Grouper(key=date_col, freq=\"1D\")).size().reset_index(name = \"pm_alert_cnt\")\n",
    "    \n",
    "    pm_data_agg[\"date\"] = pm_data_agg[date_col].dt.date\n",
    "    pm_data_agg = pm_data_agg.drop(date_col, 1)\n",
    "    \n",
    "    \n",
    "    # 전체 일자 생성\n",
    "    start_year = pm_data[date_col].dt.year.min()\n",
    "    end_year = pm_data[date_col].dt.year.max()\n",
    "    \n",
    "    date_df = pd.DataFrame({\"date\" : pd.date_range(f\"{start_year}-01-01\", f\"{end_year}-12-31\", freq = \"1D\")})\n",
    "    date_df[\"date\"] = date_df[\"date\"].dt.date\n",
    "    \n",
    "    # 미세먼지 경보일자에 Y/N 추가\n",
    "    date_df = pd.merge(date_df, pm_data_agg, on = \"date\", how = \"left\")\n",
    "    date_df[\"pm_alert_cnt\"] = np.where(date_df[\"pm_alert_cnt\"].isna(), \"N\", \"Y\")\n",
    "    \n",
    "    \n",
    "    return date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "vocal-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data = preprocess_pm_data(pm_data = pm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "controversial-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(all_date, pm_data, how = \"left\", on = \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-virtue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "limiting-fiction",
   "metadata": {},
   "source": [
    "### 상권정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "detected-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_area = pd.read_csv(\"/home/seho/Passenger_Demand/data/울산광역시_상권정보_201231.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "middle-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(latlon1, latlon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lat1, lon1 = map(radians, latlon1)\n",
    "    lat2, lon2 = map(radians, latlon2)\n",
    "#     lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def count_store_nearby(data, trading_area = trading_area, dist = 0.1, category_list = None):\n",
    "    data_copy = data.copy()\n",
    "    if category_list == None:\n",
    "        category_list = trading_area[\"상권업종중분류명\"].drop_duplicates().to_list()\n",
    "    \n",
    "    dist_list = trading_area[[\"위도\", \"경도\"]].apply(lambda x: haversine((x[\"위도\"], x[\"경도\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "    within_data = trading_area.loc[dist_list <= dist]\n",
    "    \n",
    "    \n",
    "    for i, ctgr in enumerate(category_list):\n",
    "        data_copy[f\"store_category_{i}\"] = (within_data[\"상권업종중분류명\"] == ctgr).sum()\n",
    "\n",
    "    return data_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "wooden-jesus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:09<00:00,  1.12it/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.14it/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.10it/s]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.08it/s]\n",
      " 18%|█▊        | 2/11 [00:00<00:04,  2.10it/s]]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.05it/s]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.09it/s]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.08it/s]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.10it/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.12it/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.16it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 s, sys: 983 ms, total: 18.6 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bus_stop_info = parallelize_dataframe(df = bus_stop_info, \n",
    "                                           func = count_store_nearby, \n",
    "                                           num_cores = 12, \n",
    "                                           trading_area = trading_area, \n",
    "                                           dist = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-herald",
   "metadata": {},
   "source": [
    "### 병원정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "accessible-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_data = pd.read_parquet(\"/home/seho/Passenger_Demand/data/hospital_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "objective-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_data.to_csv(\"hospital_data.csv\", encoding = \"euc-kr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "royal-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_data[\"category\"] = hospital_data[\"의료기관종별\"].replace({\"한방병원\" : \"병원\",\n",
    "                                                                  \"치과병원\" : \"병원\",\n",
    "                                                                  \"일반요양병원\" : \"요양병원\",\n",
    "                                                                  \"부속의원\" : \"의원\",\n",
    "                                                                  \"치과의원\" : \"의원\",\n",
    "                                                                  \"한의원\" : \"의원\",\n",
    "                                                                  \"보건지소\" : \"보건소\",\n",
    "                                                                  \"보건진료소\" : \"보건소\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "diverse-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_category_list = hospital_data[\"category\"].drop_duplicates().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "affiliated-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hospital_nearby(data, hospital_data = hospital_data, dist = 0.2, category_list = None):\n",
    "    data_copy = data.copy()\n",
    "    if category_list == None:\n",
    "        category_list = hospital_data[\"category\"].drop_duplicates().to_list()\n",
    "    \n",
    "    dist_list = hospital_data[[\"lat\", \"lng\"]].apply(lambda x: haversine((x[\"lat\"], x[\"lng\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "    within_data = hospital_data.loc[dist_list <= dist]\n",
    "    \n",
    "    \n",
    "    for i, ctgr in enumerate(category_list):\n",
    "        data_copy[f\"hospital_category_{i}\"] = (within_data[\"category\"] == ctgr).sum()\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "productive-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 23.78it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 24.00it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 23.93it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 22.80it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 22.79it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 22.66it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 23.19it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 23.41it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 23.66it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 22.36it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 24.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 23.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 s, sys: 0 ns, total: 1.07 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bus_stop_401_info = parallelize_dataframe(df = bus_stop_401_info, \n",
    "                                           func = count_hospital_nearby, \n",
    "                                           num_cores = 12, \n",
    "                                           hospital_data = hospital_data, \n",
    "                                           dist = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-current",
   "metadata": {},
   "source": [
    "### 학교정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "accessible-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data = pd.read_excel(\"/home/seho/Passenger_Demand/data/gv_school.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "narrative-magazine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12883, 35)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "school_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "solar-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data[\"표준일차명\"] = school_data[\"표준일차명\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "moved-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data = school_data.loc[school_data[\"표준일차명\"].str.contains(\"울산\", na=\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "clear-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "collectible-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = googlemaps.Client(key='AIzaSyBRxjIW7qfFhaVyCsc2xhk5mf1hXUSi9DI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "under-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geocode(x, gmaps):\n",
    "    try:\n",
    "        result = gmaps.geocode(x)[0][\"geometry\"][\"location\"]\n",
    "        # result = [temp[\"lat\"], temp[\"lng\"]]\n",
    "    except:\n",
    "        result = None\n",
    "    \n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "union-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data[\"category\"] = school_data[\"학교종류\"].replace({\"전문대학(3년제)\" : \"전문대학\",\n",
    "                                                          \"사내대학(전문)\" : \"전문대학\",\n",
    "                                                          \"기능대학\" : \"전문대학\",\n",
    "                                                          \"일반대학원\" : \"대학원\",\n",
    "                                                          \"전문대학원\" : \"대학원\",\n",
    "                                                          \"특수대학원\" : \"대학원\",\n",
    "                                                          \"일반고등학교\" : \"고등학교\",\n",
    "                                                          \"공업고등학교\" : \"고등학교\",\n",
    "                                                          \"상업고등학교\" : \"고등학교\",\n",
    "                                                          \"가사고등학교\" : \"고등학교\",\n",
    "                                                          \"체육고등학교\" : \"고등학교\",\n",
    "                                                          \"외국어고등학교\" : \"고등학교\",\n",
    "                                                          \"과학고등학교\" : \"고등학교\",\n",
    "                                                          \"예술고등학교\" : \"고등학교\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "unlike-horizon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 481 ms, sys: 0 ns, total: 481 ms\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "school_data[\"lat_lng\"] = school_data[\"새주소\"].apply(get_geocode, gmaps = gmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "early-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data[\"lat\"] = school_data[\"lat_lng\"].apply(lambda x: x[\"lat\"])\n",
    "school_data[\"lng\"] = school_data[\"lat_lng\"].apply(lambda x: x[\"lng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "wireless-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_school_nearby(data, school_data = school_data, dist = 0.2, category_list = None):\n",
    "    data_copy = data.copy()\n",
    "    if category_list == None:\n",
    "        category_list = school_data[\"category\"].drop_duplicates().to_list()\n",
    "    \n",
    "    dist_list = school_data[[\"lat\", \"lng\"]].apply(lambda x: haversine((x[\"lat\"], x[\"lng\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "    within_data = school_data.loc[dist_list <= dist]\n",
    "    \n",
    "    \n",
    "    for i, ctgr in enumerate(category_list):\n",
    "        data_copy[f\"school_category_{i}\"] = (within_data[\"category\"] == ctgr).sum()\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "strategic-bicycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 102.96it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 77.12it/s] \n",
      "100%|██████████| 11/11 [00:00<00:00, 76.67it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 81.10it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 79.73it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 74.05it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 75.82it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 73.45it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 74.86it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 86.17it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 81.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 747 ms, sys: 0 ns, total: 747 ms\n",
      "Wall time: 807 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bus_stop_401_info = parallelize_dataframe(df = bus_stop_401_info, \n",
    "                                              func = count_school_nearby, \n",
    "                                              num_cores = 12, \n",
    "                                              school_data = school_data, \n",
    "                                              dist = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-extraction",
   "metadata": {},
   "source": [
    "### 정류장 정보 Join(거리기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "designing-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.merge(ml_data, bus_stop_401_info.drop([\"stop_id\", \"city\"],1), on = \"mybi_stop_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "latter-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 175)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape\n",
    "# (620940, 175)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-booking",
   "metadata": {},
   "source": [
    "### 울산행사정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "driven-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = pd.read_csv(\"~/Passenger_Demand/data/ulsan_event_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "emerging-america",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 19)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "tracked-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data[\"eventStartDate\"] = pd.to_datetime(event_data[\"eventStartDate\"], format = \"%Y-%m-%d\")\n",
    "event_data[\"eventEndDate\"] = pd.to_datetime(event_data[\"eventEndDate\"], format = \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ignored-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_event_nearby(data, event_data, dist = 0.2):\n",
    "    data_copy = data.copy()\n",
    "    within_data = event_data.loc[(event_data[\"eventStartDate\"] <= data_copy[\"transdate\"]) & (event_data[\"eventEndDate\"] >= data_copy[\"transdate\"])]\n",
    "    \n",
    "    if len(within_data) == 0:\n",
    "        data_copy[f\"event_nearby\"] = 0\n",
    "    else:\n",
    "        dist_list = within_data[[\"latitude\", \"longitude\"]].apply(lambda x: haversine((x[\"latitude\"], x[\"longitude\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "        data_copy[f\"event_nearby\"] = (dist_list <= dist).sum()\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "civic-fishing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51745/51745 [03:32<00:00, 243.24it/s]\n",
      "100%|██████████| 51745/51745 [03:33<00:00, 242.30it/s]\n",
      "100%|██████████| 51745/51745 [03:38<00:00, 236.29it/s]\n",
      "100%|██████████| 51745/51745 [03:41<00:00, 234.05it/s]\n",
      "100%|██████████| 51745/51745 [03:42<00:00, 233.08it/s]\n",
      "100%|██████████| 51745/51745 [03:39<00:00, 235.81it/s]\n",
      "100%|██████████| 51745/51745 [03:41<00:00, 233.70it/s]\n",
      "100%|██████████| 51745/51745 [03:41<00:00, 233.23it/s]\n",
      "100%|██████████| 51745/51745 [03:39<00:00, 235.56it/s]\n",
      "100%|██████████| 51745/51745 [03:42<00:00, 232.61it/s]\n",
      "100%|██████████| 51745/51745 [03:41<00:00, 233.26it/s]\n",
      "100%|██████████| 51745/51745 [03:42<00:00, 232.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.8 s, sys: 0 ns, total: 32.8 s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ml_data = parallelize_dataframe(df = ml_data, \n",
    "                                func = count_event_nearby, \n",
    "                                num_cores = 12, \n",
    "                                event_data = event_data, \n",
    "                                dist = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "forced-brazilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 176)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape\n",
    "# (620940, 176)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-rental",
   "metadata": {},
   "source": [
    "### 축제 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "second-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "festival_data = pd.read_csv(\"~/Passenger_Demand/data/festival_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "convertible-feeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 18)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "festival_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "biblical-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "festival_data[\"fstvlStartDate\"] = pd.to_datetime(festival_data[\"fstvlStartDate\"], format = \"%Y-%m-%d\")\n",
    "festival_data[\"fstvlEndDate\"] = pd.to_datetime(festival_data[\"fstvlEndDate\"], format = \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "looking-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_festival_nearby(data, festival_data, dist = 0.2):\n",
    "    data_copy = data.copy()\n",
    "    within_data = festival_data.loc[(festival_data[\"fstvlStartDate\"] <= data_copy[\"transdate\"]) & (festival_data[\"fstvlEndDate\"] >= data_copy[\"transdate\"])]\n",
    "    \n",
    "    if len(within_data) == 0:\n",
    "        data_copy[f\"festival_nearby\"] = 0\n",
    "    else:\n",
    "        dist_list = within_data[[\"latitude\", \"longitude\"]].apply(lambda x: haversine((x[\"latitude\"], x[\"longitude\"]), (data_copy[\"latitude\"], data_copy[\"longitude\"])), 1)\n",
    "        data_copy[f\"festival_nearby\"] = (dist_list <= dist).sum()\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "genuine-penny",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51745/51745 [01:31<00:00, 563.83it/s]\n",
      "100%|██████████| 51745/51745 [01:32<00:00, 560.97it/s]\n",
      "100%|██████████| 51745/51745 [01:31<00:00, 563.95it/s]\n",
      "100%|██████████| 51745/51745 [01:32<00:00, 560.48it/s]\n",
      "100%|██████████| 51745/51745 [01:32<00:00, 561.49it/s]\n",
      "100%|██████████| 51745/51745 [01:31<00:00, 563.31it/s]\n",
      "100%|██████████| 51745/51745 [01:32<00:00, 561.81it/s]\n",
      "100%|██████████| 51745/51745 [01:32<00:00, 561.99it/s]\n",
      "100%|██████████| 51745/51745 [01:32<00:00, 561.17it/s]\n",
      "100%|██████████| 51745/51745 [02:26<00:00, 352.92it/s]\n",
      "100%|██████████| 51745/51745 [02:27<00:00, 349.96it/s]\n",
      "100%|██████████| 51745/51745 [02:26<00:00, 353.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 0 ns, total: 1min 17s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ml_data = parallelize_dataframe(df = ml_data, \n",
    "                                func = count_festival_nearby, \n",
    "                                num_cores = 12, \n",
    "                                festival_data = festival_data, \n",
    "                                dist = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "composed-waters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620940, 177)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.shape\n",
    "# (620940, 177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "swiss-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data.drop([\"weekofyear\", \"date\"], 1).to_pickle(\"/home/seho/Passenger_Demand/data/ml_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-resistance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-mailing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-witness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-thirty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "organic-terry",
   "metadata": {},
   "source": [
    "### 인구 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_data = pd.read_csv(\"~/Passenger_Demand/data/울산광역시_인구 현황_20200727.csv\", encoding = \"euc-kr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = googlemaps.Client(key='AIzaSyBRxjIW7qfFhaVyCsc2xhk5mf1hXUSi9DI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = requests.get(\"https://maps.googleapis.com/maps/api/geocode/json?latlng=35.60467,129.4328&key=AIzaSyBRxjIW7qfFhaVyCsc2xhk5mf1hXUSi9DI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://maps.googleapis.com/maps/api/geocode/json?latlng=35.60467,129.4328&key=AIzaSyBRxjIW7qfFhaVyCsc2xhk5mf1hXUSi9DI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps.reverse_geocode((35.60467, 129.4328), language = \"korean\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
